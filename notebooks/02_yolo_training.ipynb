{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¯ YOLOv8 Training for Pallet Box Detection\n",
        "\n",
        "This notebook trains YOLOv8 object detection models for counting boxes on pallets.\n",
        "\n",
        "**Training Plan:**\n",
        "1. Train YOLOv8n (nano) - fast baseline\n",
        "2. Train YOLOv8s (small) - improved accuracy\n",
        "3. Compare models and select best\n",
        "4. Optimize confidence threshold for counting accuracy\n",
        "\n",
        "**Dataset:**\n",
        "- Combined: 6,875 images (70/20/10 split)\n",
        "- Train: 4,812 | Valid: 1,374 | Test: 689\n",
        "- Single class: `box`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# ML Libraries\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "# Utilities\n",
        "import yaml\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Set paths\n",
        "PROJECT_ROOT = Path('../').resolve()\n",
        "DATA_CONFIG = PROJECT_ROOT / 'data' / 'combined_data.yaml'\n",
        "MODELS_DIR = PROJECT_ROOT / 'models'\n",
        "\n",
        "# Check GPU availability\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f\"âœ“ Using device: {device}\")\n",
        "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
        "print(f\"âœ“ Data config: {DATA_CONFIG}\")\n",
        "print(f\"âœ“ Models will be saved to: {MODELS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data configuration\n",
        "with open(DATA_CONFIG, 'r') as f:\n",
        "    data_config = yaml.safe_load(f)\n",
        "    \n",
        "print(\"Data Configuration:\")\n",
        "print(f\"  Train paths: {data_config.get('train', 'N/A')}\")\n",
        "print(f\"  Val paths: {data_config.get('val', 'N/A')}\")\n",
        "print(f\"  Test paths: {data_config.get('test', 'N/A')}\")\n",
        "print(f\"  Classes: {data_config.get('names', 'N/A')}\")\n",
        "print(f\"  Number of classes: {data_config.get('nc', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train YOLOv8n (Nano) - Baseline Model\n",
        "\n",
        "Fast, lightweight model for initial baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "IMG_SIZE = 640\n",
        "PATIENCE = 10  # Early stopping patience\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING YOLOV8N (NANO) - BASELINE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Image Size: {IMG_SIZE}\")\n",
        "print(f\"Early Stopping Patience: {PATIENCE}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize YOLOv8n model\n",
        "model_nano = YOLO('yolov8n.pt')  # Load pretrained weights\n",
        "\n",
        "# Train the model\n",
        "results_nano = model_nano.train(\n",
        "    data=str(DATA_CONFIG),\n",
        "    epochs=EPOCHS,\n",
        "    batch=BATCH_SIZE,\n",
        "    imgsz=IMG_SIZE,\n",
        "    patience=PATIENCE,\n",
        "    device=device,\n",
        "    project=str(PROJECT_ROOT / 'runs' / 'detect'),\n",
        "    name='yolov8n_boxes',\n",
        "    exist_ok=True,\n",
        "    verbose=True,\n",
        "    plots=True,\n",
        "    save=True,\n",
        "    val=True\n",
        ")\n",
        "\n",
        "print(\"\\nâœ“ YOLOv8n training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "best_model_path = PROJECT_ROOT / 'runs' / 'detect' / 'yolov8n_boxes' / 'weights' / 'best.pt'\n",
        "target_path = MODELS_DIR / 'yolov8n_boxes_best.pt'\n",
        "\n",
        "if best_model_path.exists():\n",
        "    shutil.copy(best_model_path, target_path)\n",
        "    print(f\"âœ“ Best model saved to: {target_path}\")\n",
        "else:\n",
        "    print(f\"âš  Best model not found at: {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train YOLOv8s (Small) - Improved Model\n",
        "\n",
        "Larger model for potentially better accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TRAINING YOLOV8S (SMALL) - IMPROVED MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize YOLOv8s model\n",
        "model_small = YOLO('yolov8s.pt')  # Load pretrained weights\n",
        "\n",
        "# Train the model\n",
        "results_small = model_small.train(\n",
        "    data=str(DATA_CONFIG),\n",
        "    epochs=EPOCHS,\n",
        "    batch=BATCH_SIZE,\n",
        "    imgsz=IMG_SIZE,\n",
        "    patience=PATIENCE,\n",
        "    device=device,\n",
        "    project=str(PROJECT_ROOT / 'runs' / 'detect'),\n",
        "    name='yolov8s_boxes',\n",
        "    exist_ok=True,\n",
        "    verbose=True,\n",
        "    plots=True,\n",
        "    save=True,\n",
        "    val=True\n",
        ")\n",
        "\n",
        "print(\"\\nâœ“ YOLOv8s training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "best_model_path = PROJECT_ROOT / 'runs' / 'detect' / 'yolov8s_boxes' / 'weights' / 'best.pt'\n",
        "target_path = MODELS_DIR / 'yolov8s_boxes_best.pt'\n",
        "\n",
        "if best_model_path.exists():\n",
        "    shutil.copy(best_model_path, target_path)\n",
        "    print(f\"âœ“ Best model saved to: {target_path}\")\n",
        "else:\n",
        "    print(f\"âš  Best model not found at: {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation & Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained models for evaluation\n",
        "model_nano_eval = YOLO(str(MODELS_DIR / 'yolov8n_boxes_best.pt'))\n",
        "model_small_eval = YOLO(str(MODELS_DIR / 'yolov8s_boxes_best.pt'))\n",
        "\n",
        "print(\"âœ“ Models loaded for evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate both models on validation set\n",
        "print(\"Evaluating YOLOv8n on validation set...\")\n",
        "metrics_nano = model_nano_eval.val(data=str(DATA_CONFIG), split='val')\n",
        "\n",
        "print(\"\\nEvaluating YOLOv8s on validation set...\")\n",
        "metrics_small = model_small_eval.val(data=str(DATA_CONFIG), split='val')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model metrics\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL COMPARISON - DETECTION METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison_data = {\n",
        "    'Model': ['YOLOv8n (Nano)', 'YOLOv8s (Small)'],\n",
        "    'mAP@0.5': [metrics_nano.box.map50, metrics_small.box.map50],\n",
        "    'mAP@0.5:0.95': [metrics_nano.box.map, metrics_small.box.map],\n",
        "    'Precision': [metrics_nano.box.mp, metrics_small.box.mp],\n",
        "    'Recall': [metrics_nano.box.mr, metrics_small.box.mr]\n",
        "}\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "# Determine best model\n",
        "best_model_name = 'YOLOv8n' if metrics_nano.box.map50 >= metrics_small.box.map50 else 'YOLOv8s'\n",
        "print(f\"\\nâœ“ Best model by mAP@0.5: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Counting Accuracy Evaluation\n",
        "\n",
        "The business metric is counting accuracy, not just detection metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_counting_accuracy(model, data_path, split='test', conf_threshold=0.4):\n",
        "    \"\"\"\n",
        "    Evaluate counting accuracy on a dataset split.\n",
        "    Returns counting metrics rather than detection metrics.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    \n",
        "    # Get image and label paths\n",
        "    data_root = Path(data_path).parent.parent\n",
        "    \n",
        "    # Collect test images from both datasets\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "    \n",
        "    for dataset_name in ['Boxes.v1i.yolov8', 'Final_Object_Detection.v1i.yolov8']:\n",
        "        dataset_path = data_root / dataset_name\n",
        "        images_dir = dataset_path / split / 'images'\n",
        "        labels_dir = dataset_path / split / 'labels'\n",
        "        \n",
        "        if images_dir.exists():\n",
        "            for img_path in images_dir.glob('*.jpg'):\n",
        "                label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
        "                if label_path.exists():\n",
        "                    test_images.append(img_path)\n",
        "                    test_labels.append(label_path)\n",
        "    \n",
        "    results = []\n",
        "    for img_path, label_path in zip(test_images, test_labels):\n",
        "        # Get ground truth count\n",
        "        with open(label_path, 'r') as f:\n",
        "            true_count = len([l for l in f.readlines() if l.strip()])\n",
        "        \n",
        "        # Get predicted count\n",
        "        pred = model.predict(str(img_path), conf=conf_threshold, verbose=False)\n",
        "        pred_count = len(pred[0].boxes) if pred[0].boxes is not None else 0\n",
        "        \n",
        "        results.append({\n",
        "            'image': img_path.name,\n",
        "            'true_count': true_count,\n",
        "            'pred_count': pred_count,\n",
        "            'error': pred_count - true_count,\n",
        "            'abs_error': abs(pred_count - true_count)\n",
        "        })\n",
        "    \n",
        "    df_results = pd.DataFrame(results)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    exact_match = (df_results['true_count'] == df_results['pred_count']).mean()\n",
        "    off_by_1 = (df_results['abs_error'] <= 1).mean()\n",
        "    off_by_2 = (df_results['abs_error'] <= 2).mean()\n",
        "    mae = df_results['abs_error'].mean()\n",
        "    \n",
        "    return {\n",
        "        'count_accuracy': exact_match,\n",
        "        'off_by_1': off_by_1,\n",
        "        'off_by_2': off_by_2,\n",
        "        'mae': mae,\n",
        "        'results_df': df_results\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Counting accuracy function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate counting accuracy for both models on test set\n",
        "print(\"=\"*60)\n",
        "print(\"COUNTING ACCURACY EVALUATION (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# YOLOv8n counting accuracy\n",
        "print(\"\\nEvaluating YOLOv8n counting accuracy...\")\n",
        "count_metrics_nano = evaluate_counting_accuracy(model_nano_eval, str(DATA_CONFIG), split='test', conf_threshold=0.4)\n",
        "\n",
        "# YOLOv8s counting accuracy  \n",
        "print(\"Evaluating YOLOv8s counting accuracy...\")\n",
        "count_metrics_small = evaluate_counting_accuracy(model_small_eval, str(DATA_CONFIG), split='test', conf_threshold=0.4)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COUNTING METRICS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "count_comparison = {\n",
        "    'Model': ['YOLOv8n', 'YOLOv8s'],\n",
        "    'Count Accuracy': [f\"{count_metrics_nano['count_accuracy']*100:.1f}%\", f\"{count_metrics_small['count_accuracy']*100:.1f}%\"],\n",
        "    'Off-by-1 Accuracy': [f\"{count_metrics_nano['off_by_1']*100:.1f}%\", f\"{count_metrics_small['off_by_1']*100:.1f}%\"],\n",
        "    'Off-by-2 Accuracy': [f\"{count_metrics_nano['off_by_2']*100:.1f}%\", f\"{count_metrics_small['off_by_2']*100:.1f}%\"],\n",
        "    'Count MAE': [f\"{count_metrics_nano['mae']:.2f}\", f\"{count_metrics_small['mae']:.2f}\"]\n",
        "}\n",
        "\n",
        "df_count_comparison = pd.DataFrame(count_comparison)\n",
        "print(df_count_comparison.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Confidence Threshold Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find optimal confidence threshold using the better model\n",
        "# Use YOLOv8s for threshold optimization\n",
        "best_model = model_small_eval\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CONFIDENCE THRESHOLD OPTIMIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "threshold_results = []\n",
        "\n",
        "for thresh in thresholds:\n",
        "    print(f\"Testing threshold: {thresh}...\")\n",
        "    metrics = evaluate_counting_accuracy(best_model, str(DATA_CONFIG), split='valid', conf_threshold=thresh)\n",
        "    threshold_results.append({\n",
        "        'threshold': thresh,\n",
        "        'count_accuracy': metrics['count_accuracy'],\n",
        "        'off_by_1': metrics['off_by_1'],\n",
        "        'mae': metrics['mae']\n",
        "    })\n",
        "\n",
        "df_thresholds = pd.DataFrame(threshold_results)\n",
        "print(\"\\nThreshold Analysis:\")\n",
        "print(df_thresholds.to_string(index=False))\n",
        "\n",
        "# Find best threshold\n",
        "best_idx = df_thresholds['count_accuracy'].idxmax()\n",
        "best_threshold = df_thresholds.loc[best_idx, 'threshold']\n",
        "print(f\"\\nâœ“ Optimal confidence threshold: {best_threshold}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot threshold analysis\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Count accuracy vs threshold\n",
        "ax1 = axes[0]\n",
        "ax1.plot(df_thresholds['threshold'], df_thresholds['count_accuracy'] * 100, 'b-o', linewidth=2, markersize=8)\n",
        "ax1.axvline(best_threshold, color='red', linestyle='--', label=f'Best: {best_threshold}')\n",
        "ax1.set_xlabel('Confidence Threshold', fontsize=12)\n",
        "ax1.set_ylabel('Count Accuracy (%)', fontsize=12)\n",
        "ax1.set_title('Count Accuracy vs Confidence Threshold', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Off-by-1 accuracy vs threshold\n",
        "ax2 = axes[1]\n",
        "ax2.plot(df_thresholds['threshold'], df_thresholds['off_by_1'] * 100, 'g-o', linewidth=2, markersize=8)\n",
        "ax2.axvline(best_threshold, color='red', linestyle='--', label=f'Best: {best_threshold}')\n",
        "ax2.set_xlabel('Confidence Threshold', fontsize=12)\n",
        "ax2.set_ylabel('Off-by-1 Accuracy (%)', fontsize=12)\n",
        "ax2.set_title('Off-by-1 Accuracy vs Confidence Threshold', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# MAE vs threshold\n",
        "ax3 = axes[2]\n",
        "ax3.plot(df_thresholds['threshold'], df_thresholds['mae'], 'r-o', linewidth=2, markersize=8)\n",
        "ax3.axvline(best_threshold, color='blue', linestyle='--', label=f'Best: {best_threshold}')\n",
        "ax3.set_xlabel('Confidence Threshold', fontsize=12)\n",
        "ax3.set_ylabel('Count MAE', fontsize=12)\n",
        "ax3.set_title('MAE vs Confidence Threshold', fontsize=14, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/threshold_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Figure saved to docs/threshold_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Sample Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get sample test images\n",
        "test_images_dir = PROJECT_ROOT / 'Boxes.v1i.yolov8' / 'test' / 'images'\n",
        "sample_images = list(test_images_dir.glob('*.jpg'))[:6]\n",
        "\n",
        "# Run predictions\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "for ax, img_path in zip(axes.flatten(), sample_images):\n",
        "    # Run prediction\n",
        "    results = best_model.predict(str(img_path), conf=best_threshold, verbose=False)\n",
        "    \n",
        "    # Get annotated image\n",
        "    annotated_img = results[0].plot()\n",
        "    annotated_img = annotated_img[:, :, ::-1]  # BGR to RGB\n",
        "    \n",
        "    # Get count\n",
        "    pred_count = len(results[0].boxes) if results[0].boxes is not None else 0\n",
        "    \n",
        "    ax.imshow(annotated_img)\n",
        "    ax.set_title(f'Predicted: {pred_count} boxes', fontsize=12, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Sample Predictions with Best Model', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/sample_predictions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Figure saved to docs/sample_predictions.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training summary\n",
        "training_summary = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'models_trained': ['yolov8n', 'yolov8s'],\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'image_size': IMG_SIZE,\n",
        "    'device': device,\n",
        "    'yolov8n_metrics': {\n",
        "        'mAP50': float(metrics_nano.box.map50),\n",
        "        'mAP50_95': float(metrics_nano.box.map),\n",
        "        'precision': float(metrics_nano.box.mp),\n",
        "        'recall': float(metrics_nano.box.mr),\n",
        "        'count_accuracy': count_metrics_nano['count_accuracy'],\n",
        "        'count_mae': count_metrics_nano['mae']\n",
        "    },\n",
        "    'yolov8s_metrics': {\n",
        "        'mAP50': float(metrics_small.box.map50),\n",
        "        'mAP50_95': float(metrics_small.box.map),\n",
        "        'precision': float(metrics_small.box.mp),\n",
        "        'recall': float(metrics_small.box.mr),\n",
        "        'count_accuracy': count_metrics_small['count_accuracy'],\n",
        "        'count_mae': count_metrics_small['mae']\n",
        "    },\n",
        "    'best_model': 'yolov8s' if count_metrics_small['count_accuracy'] >= count_metrics_nano['count_accuracy'] else 'yolov8n',\n",
        "    'optimal_confidence_threshold': float(best_threshold)\n",
        "}\n",
        "\n",
        "with open('../docs/training_summary.json', 'w') as f:\n",
        "    json.dump(training_summary, f, indent=2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"                    TRAINING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "ðŸ“Š MODELS TRAINED\n",
        "{'â”€'*50}\n",
        "â€¢ YOLOv8n (Nano) - Baseline\n",
        "â€¢ YOLOv8s (Small) - Improved\n",
        "\n",
        "ðŸ“ˆ BEST MODEL: {training_summary['best_model'].upper()}\n",
        "{'â”€'*50}\n",
        "â€¢ mAP@0.5: {training_summary[f\"{training_summary['best_model']}_metrics\"]['mAP50']:.4f}\n",
        "â€¢ Count Accuracy: {training_summary[f\"{training_summary['best_model']}_metrics\"]['count_accuracy']*100:.1f}%\n",
        "â€¢ Count MAE: {training_summary[f\"{training_summary['best_model']}_metrics\"]['count_mae']:.2f}\n",
        "â€¢ Optimal Threshold: {best_threshold}\n",
        "\n",
        "ðŸ’¾ SAVED MODELS\n",
        "{'â”€'*50}\n",
        "â€¢ models/yolov8n_boxes_best.pt\n",
        "â€¢ models/yolov8s_boxes_best.pt\n",
        "\n",
        "âœ“ Training summary saved to docs/training_summary.json\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"                    YOLOv8 TRAINING COMPLETE âœ“\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
